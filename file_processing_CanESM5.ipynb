{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "import cftime\n",
    "from future_wind_copy import combine_hemispheres \n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_europe_and_interpolate(ds_rsds,ds_rsdsdiff,ds_tas):\n",
    "    \n",
    "\n",
    "    ds_rsds_europe = combine_hemispheres(ds_rsds,minlat=20.,maxlat=75.,minlon=330.,maxlon=50.)\n",
    "    ds_rsdsdiff_europe = combine_hemispheres(ds_rsdsdiff,minlat=20.,maxlat=75.,minlon=330.,maxlon=50.)    \n",
    "    ds_tas_europe = combine_hemispheres(ds_tas,minlat=20.,maxlat=75.,minlon=330.,maxlon=50.) \n",
    "\n",
    "    # Interpolate tas to match rsds time\n",
    "    tas_interp= ds_tas_europe['tas'].interp(time=ds_rsds['time'], method=\"linear\")\n",
    "    # Create a mask for the NaN values using .isnull()\n",
    "    nan_mask = tas_interp.isnull()\n",
    "\n",
    "    # For each time step, replace NaNs with the values from the next time step\n",
    "    for t in range(len(tas_interp.time) - 1):  # Exclude the last time step\n",
    "        # Use .isel() to ensure the correct alignment of coordinates\n",
    "        tas_interp[t] = tas_interp[t].where(~nan_mask[t], tas_interp.isel(time=t + 1))\n",
    "    \n",
    "    ds_tas_europe['tas'] = tas_interp\n",
    "    ds_tas_europe['time'] = ds_rsds_europe['time']\n",
    "\n",
    "    return ds_rsds_europe, ds_rsdsdiff_europe, ds_tas_europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_set(du,rsds,rsdsdiff,tas):\n",
    "    \"\"\"Creates xarray DataArray for netCDF write\n",
    "\n",
    "    Args:\n",
    "        du (dataset): sample dataset with attributes\n",
    "        rsds (DataArray): wind speed \n",
    "        rsdsdiff (DataArray): wind direction\n",
    "        tas (DataArray): surface air density\n",
    "\n",
    "    Returns:\n",
    "        xarray DataArray: DataArray for write\n",
    "    \"\"\"\n",
    "    lat = xr.DataArray(\n",
    "        data=rsds.lat.values.astype('float32'),\n",
    "        dims=[\"lat\"],\n",
    "        coords=dict(\n",
    "            lat=([\"lat\"], rsds.lat.values)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "        long_name=\"latitude\",\n",
    "        units=\"degrees_north\",\n",
    "        axis=\"Y\"\n",
    "        ),\n",
    "    )\n",
    "    lon = xr.DataArray(\n",
    "        data=rsds.lon.values.astype('float32'),\n",
    "        dims=\"lon\",\n",
    "        coords=dict(\n",
    "            lon=([\"lon\"], rsds.lon.values)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "        long_name=\"longitude\",\n",
    "        units=\"degrees_east\",\n",
    "        axis=\"X\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            rsds = (\n",
    "                [\"time\",\"lat\",\"lon\"],rsds.values.astype('float32'),\n",
    "                dict(long_name = \"rsds\",\n",
    "                units = \"W/m2\")),\n",
    "            rsdsdiff = (\n",
    "                [\"time\",\"lat\",\"lon\"],rsdsdiff.values.astype('float32'),\n",
    "                dict(long_name = \"rsdsdiff\",\n",
    "                units = \"W/m2\",\n",
    "                vert_units = \"W/m2\")),\n",
    "            tas = (\n",
    "                [\"time\",\"lat\",\"lon\"],tas.values.astype('float32'),\n",
    "                dict(long_name = \"surface air density\",\n",
    "                units = \"K\",\n",
    "                height = \"surface\")),\n",
    "            ),\n",
    "        coords=dict(\n",
    "            lon=lon,\n",
    "            lat=lat,\n",
    "            time=rsds.time\n",
    "            ),\n",
    "        attrs=dict(\n",
    "            data_source = \"Processed data from CMIP6 runs\",\n",
    "            experiment = du.experiment_id,\n",
    "            source = du.source_id,\n",
    "            variant_label = du.variant_label,\n",
    "            data_written = datetime.now().strftime(\"%d/%m/%Y %H:%M\")\n",
    "            )\n",
    "    )   \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_path_to_file(root_url,model,experiment,variant,table,var,grid,version,dates):\n",
    "\n",
    "    root_file = \"/\".join((root_url,experiment,variant,table,var,grid,version))\n",
    "    filename = \"_\".join((var,table,model,experiment,variant,grid,dates))+\".nc\"\n",
    "    path_to_file = \"/\".join((root_file,filename))\n",
    "    return(path_to_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = \"CanESM5\"\n",
    "    root_url = \"/groups/FutureWind/SFCRAD/CanESM5/historical/r1i1p2f1/\"\n",
    "    #\"http://crd-esgf-drc.ec.gc.ca/thredds/dodsC/esgI_dataroot/AR6/CMIP6/ScenarioMIP/CCCma/CanESM5-1\"\n",
    "    # ssp585/r1i1p2f1/6hrLev/ua/gn/v20190429/ua_6hrLev_CanESM5-1_ssp585_r1i1p2f1_gn_205101010000-205112311800.nc\"\n",
    "    \n",
    "    grid = \"gn\"\n",
    "    version = \"v20190429\"\n",
    "    \n",
    "    calendar = 'noLeap'\n",
    "    experiment = sys.argv[1]\n",
    "    if (experiment == \"historical\"):\n",
    "        year = 1980; last_year = 2014\n",
    "    else:\n",
    "        # year = 2015; last_year = 2050\n",
    "        year = 2050; last_year = 2070\n",
    "    variant = sys.argv[2]  \n",
    "    print(\"Retrieve data for\",\\\n",
    "        \"\\n model:  \",model,\"\\n experiment:\",experiment,\\\n",
    "        \"\\n variant:\",variant)\n",
    "\n",
    "    # What filenames already exist in the directory\n",
    "    filenames = \"rsds_rsdsdiff_tas_????.nc\"\n",
    "    old_files = sorted(glob(filenames))\n",
    "\n",
    "    if not old_files:    # This is necessary for the scenario files that start at 00Z\n",
    "        print(\"No previous files\")\n",
    "        month = 1\n",
    "        date = cftime.datetime(year,1,1,6,calendar=calendar) # Files start at 06 not 00\n",
    "    else:\n",
    "        ff = xr.open_dataset(old_files[-1],decode_times=True,use_cftime=True)\n",
    "        date = ff.time[-1] + timedelta(hours=6)\n",
    "        print(\"Next date:\",date.values)\n",
    "        year = date.dt.year.values\n",
    "        month = date.dt.month.values\n",
    "        date = datetime_to_cftime(date,calendar=calendar)\n",
    "\n",
    "    print(\"year\",year)\n",
    "    last_date = cftime.datetime(last_year+1,1,1,0,calendar=calendar)\n",
    "    print(date,last_date)\n",
    "\n",
    "    while (year <= last_year):\n",
    "\n",
    "        # What is the date string in the files, 1 year each\n",
    "        start_date = cftime.datetime(year,1,1,0,calendar=calendar)\n",
    "        end_date = cftime.datetime(year,12,31,18,calendar=calendar)\n",
    "        dates = \"-\".join((start_date.strftime(\"%Y%m%d%H%M\"),end_date.strftime(\"%Y%m%d%H%M\")))\n",
    "        print(\"Yearly file dates\",dates)\n",
    "\n",
    "        # Find the file where next_day is found \n",
    "        var = \"rsds\"; table = \"3hr\"\n",
    "        path_to_file = make_path_to_file(root_url,model,experiment,variant,table,var,grid,version,dates)\n",
    "        print(\"file open:\",path_to_file)\n",
    "        ds_rsds = xr.open_dataset(path_to_file,decode_times=True,use_cftime=True)\n",
    "        \n",
    "        var = \"rsdsdiff\"; table = \"3hr\"\n",
    "        path_to_file = make_path_to_file(root_url,model,experiment,variant,table,var,grid,version,dates)\n",
    "        print(\"file open:\",path_to_file)\n",
    "        ds_rsdsdiff = xr.open_dataset(path_to_file,decode_times=True,use_cftime=True)\n",
    "\n",
    "        var = \"tas\"; table = \"3hr\"\n",
    "        path_to_file = make_path_to_file(root_url,model,experiment,variant,table,var,grid,version,dates)\n",
    "        print(\"file open:\",path_to_file)\n",
    "        ds_tas = xr.open_dataset(path_to_file,decode_times=True,use_cftime=True)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(12):\n",
    "            date = cftime.datetime(year,month,1,0,calendar=calendar)\n",
    "            year = year + month // 12\n",
    "            month = month % 12 + 1\n",
    "            print(year,month)\n",
    "            date_end = cftime.datetime(year,month,1,0,calendar=calendar) - timedelta(hours=6)\n",
    "            print(date,date_end)\n",
    "\n",
    "            ws,wd,rho,filename = read_and_interp(\n",
    "                dt,dq,du,dv,slice(date,date_end))\n",
    "\n",
    "            ds = fw.make_data_set(du,ws,wd,rho)\n",
    "            ds.to_netcdf(filename,mode=\"w\",engine=\"netcdf4\",\n",
    "                        unlimited_dims='time')\n",
    "            print(filename,\" written to disk\")\n",
    "                \n",
    "        # date = cftime.datetime(year,1,1,6,calendar=calendar)\n",
    "        # print(\"Next date:\",date.strftime(\"%Y-%m-%d_%H\"),\"year:\",year)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlite_cmip6_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
